"""Code written by Lauren F. Heald with support from Caidan Moore, Cuong Le, and Yusong Liu. Intended to be used for processing ultrafast 
gas phase electron diffraction data collected at the MeV-UED hutch of LCLS at the Stanford Linear Accelerator National Laboratory. 

Questions or Concerns: 
    email: lheald2@unl.edu

Affiliations:
    Centurion Lab at the University of Nebraska -- Lincoln, NE
    Stanford Linear Accelerator National Lab -- Menlo Park, CA
"""


# Standard Packages
import numpy as np
import numpy.ma as ma
from tifffile import tifffile as tf
import matplotlib.pyplot as plt
import matplotlib
import pandas as pd
import scipy.signal as ss
import concurrent.futures
from functools import partial
import h5py

# Image stuff
import matplotlib.patches as patches
from skimage.filters import threshold_otsu
from skimage.morphology import closing, square, disk
from skimage.segmentation import clear_border
from skimage.measure import label, regionprops_table
from skimage import util, draw

# Configuration File
from gued_globals import *


### Reading Images Functions

def _show_counts(stage_positions, counts):
    """Function for visualizing and plotting total counts from a set of data. Called within the get_image_details
    function when plot == True"""

    counts_mean = np.mean(counts)  # Mean values of Total Counts of all images
    counts_std = np.std(counts)  # the STD of all the tc for all the iamges
    uni_stage = np.unique(stage_positions)  # Pump-probe stage position
    plt.figure()  # Plot counts rate, images number at each posi, and bad images

    plt.subplot(1, 3, 1)
    plt.plot(counts, '-d')
    plt.axhline(y=counts_mean, color='k', linestyle='-', linewidth=1, label="mean counts");
    plt.axhline(y=counts_mean - (3 * counts_std), color='r', linestyle='-', linewidth=0.5, label="min counts");
    plt.axhline(y=counts_mean + (3 * counts_std), color='r', linestyle='-', linewidth=0.5, label="max counts");
    plt.xlabel('Images orderd in lab time');
    plt.ylabel('Counts');
    plt.legend()
    plt.title('Total counts');

    plt.subplot(1, 3, 2)  # Histogram the number of images at each posi
    plt.plot(uni_stage, '-o');
    plt.xlabel('pp stage posi');
    plt.ylabel('Stg Position [mm]');
    plt.title('Delay Stage Position');

    plt.subplot(1, 3, 3)  # Histogram the number of images at each posi
    posi_edges_bins = np.append(uni_stage - 0.001, uni_stage[-1])
    posi_hist, posi_edges = np.histogram(stage_positions, bins=posi_edges_bins)
    plt.plot(uni_stage, posi_hist, '-*')
    plt.xlabel('pp stage posi [mm]');
    plt.ylabel('Num of Imges');
    plt.title('Num of images at each delay');

    plt.tight_layout()
    plt.show()


def _get_counts(data_array, plot=False):
    """
    Generates the counts from the given data by summing over the array elements. Returns 2d array of the same dimension as the
    input images.

    ARGUMENTS:

    data_array (3d array): 
        Array containing the diffraction images.
    
    OPTIONAL ARGUMENTS:

    plot (boolean): 
        Default set to False. When true, plots a graph of the counts data.

    RETURNS:

    counts (2d array): 
        One dimensional array containing the data after summing over each array element.

    """
    counts = np.sum(data_array, axis=(1, 2))
    if len(data_array) == 0:
        raise ValueError("Input data_array is empty.")
    if data_array.ndim != 3:
        raise ValueError("Input data_array is not 3 dimensional.")
    if plot == True:
        plt.plot(np.arange(len(data_array[:, 0, 0])), counts)
        plt.show()
    return counts


def get_image_details(file_names, sort=True, filter_data=False, plot=False):
    """
    Reads all images from input file_names and returns the data as a 3d array along with stage positions, order, and counts per image.

    ARGUMENTS:

    file_names (list):
        list of file names to be read in

    OPTIONAL ARGUMENTS:

    sort (boolean): 
        default is set to True. This arguments sorts the data based on when it was saved (i.e. file number)
    plot (boolean): 
        default is set to False. When True, a plot of the data, log(data), and histogram of counts is shown
    filter_data (boolean): 
        default is set to False. When True, code prompts you for a minimum and maximum value then
        returns only the information from files within this range

    GLOBAL VARIABLES:

    SEPARATORS (list):
        list of strings such as '_' or '-' which are used in the file naming scheme to separate values needed for data analysis (i.e. stage
        position)

    RETURNS:

    data_array (3d array): 
        Array of N x 1024 x 1024 where N is the length of tile file_names list. Generated by using tifffile as tf.
    stage_positions (array): 
        An array containing the stage positions of the file. The index of each stage position corresponds to the index of the file name 
        in file_names.
    file_order (array): 
        Returns the image number located in the file name. Reflects the order with which the images are taken.
    counts(array): 
        One dimensional numpy array of length N containing the data after summing over each array element.

    """
    data_array = tf.imread(file_names)  # construct array containing files
    if len(SEPARATORS) == 2:
        try:
            stage_pos = []
            file_order = []
            try:
                # stage_pos = [np.float64(file_name[idx_start:idx_end]) for file_name in file_names]
                # stage_pos = np.array(stage_pos)
                for file in file_names:
                    string = list(
                        map(str, file.split("\\")))  # Note standard slash usage for windows todo might need to test
                    folder_number = string[-3][-3:]
                    string = list(map(str, string[-1].split(SEPARATORS[0])))
                    file_number = int(folder_number + string[1])
                    file_order.append(int(file_number))
                    string = list(map(str, string[-1].split(SEPARATORS[1])))
                    stage_pos.append(float(string[0]))
            except ValueError:
                raise ValueError("""Failed to convert a file name to a float. Make sure that index positions are correct for all files in file_names. 
                Also check separators""")
        except IndexError:
            raise ValueError(
                "Invalid index values. Make sure the index values are within the range of the file name strings.")
    elif len(SEPARATORS) == 1:
        try:
            stage_pos = []
            file_order = []
            try:
                # stage_pos = [np.float64(file_name[idx_start:idx_end]) for file_name in file_names]
                # stage_pos = np.array(stage_pos)
                for file in file_names:
                    string = list(map(str, file.split("/")))
                    string = list(map(str, string[-1].split(SEPARATORS)))
                    file_order.append(int(string[2]))
                    stage_pos.append(float(string[3]))
            except ValueError:
                raise ValueError(
                    """Failed to convert a file name to a float. Make sure that index positions are correct for all files in file_names. Also check separators""")
        except IndexError:
            raise ValueError(
                "Invalid index values. Make sure the index values are within the range of the file name strings.")
    else:
        print("Provide valid SEPARATORS")

    stage_pos = np.array(stage_pos)
    file_order = np.array(file_order)
    counts = _get_counts(data_array)

    if sort == True:
        idx_sort = np.argsort(file_order)
        file_order = file_order[idx_sort]
        data_array = data_array[idx_sort]
        stage_pos = stage_pos[idx_sort]
        counts = counts[idx_sort]

    if filter_data == True:
        min_val = int(input("Enter minimum file number: "))
        max_val = int(input("Enter maximum file number: "))
        try:
            good_range = np.arange(min_val, max_val, 1)
            data_array = data_array[good_range]
            stage_pos = stage_pos[good_range]
            counts = counts[good_range]
            file_order = file_order[good_range]
        except:
            print("Max value is larger than the size of the data range, returning all data")

    if plot == True:
        test = data_array[0]
        plt.figure(figsize=[12, 10])
        plt.subplot(1, 3, 1);
        plt.imshow(test, cmap='jet');
        plt.xlabel('Pixel');
        plt.ylabel('Pixel');
        plt.title('Linear Scale(data)')

        plt.subplot(1, 3, 2);
        plt.imshow(np.log(test), cmap='jet');
        plt.xlabel('Pixel');
        plt.ylabel('Pixel');
        plt.title('Log Scale(data)')

        plt.subplot(1, 3, 3);
        plt.hist(test.reshape(-1), bins=30, edgecolor="r", histtype="bar", alpha=0.5)
        plt.xlabel('Pixel Intensity');
        plt.ylabel('Pixel Number');
        plt.title('Hist of the pixel intensity(data)');
        plt.yscale('log')
        plt.tight_layout()
        plt.show()

        _show_counts(stage_pos, counts)

    return data_array, stage_pos, file_order, counts


def get_image_details_keV(file_names, sort=False, multistage=False):
    # todo update to look like other get_image_details code and make for one stage
    """
    Reads all images from input file_names and returns the data as a 3d array along with stage positions, order, and counts per image.

    ARGUMENTS:

    file_names (list):
        list of file names to be read in

    OPTIONAL ARGUMENTS:

    sort (boolean): 
        default is set to True. This arguments sorts the data based on when it was saved (i.e. file number)
    plot (boolean): 
        default is set to False. When True, a plot of the data, log(data), and histogram of counts is shown
    filter_data (boolean): 
        default is set to False. When True, code prompts you for a minimum and maximum value then
        returns only the information from files within this range

    RETURNS:

    data_array (3d array): 
        Array of N x 1024 x 1024 where N is the length of tile file_names list. Generated by using tifffile as tf.
    stage_positions (array): 
        An array containing the stage positions of the file. The index of each stage position corresponds to the index of the file name 
        in file_names.
    file_order (array): 
        Returns the image number located in the file name. Reflects the order with which the images are taken.
    counts(array): 
        One dimensional numpy array of length N containing the data after summing over each array element.

    """
    data_array = tf.imread(file_names)  # construct array containing files
    if multistage == True:
        try:
            ir_stage_pos = []
            uv_stage_pos = []
            file_order = []
            current = []
            try:
                for file in file_names:
                    string = list(map(str, file.split("/")))
                    string = list(map(str, string[-1].split("_")))
                    file_number = int(string[1])
                    file_order.append(file_number)
                    ir_stage_pos.append(float(string[4]))
                    uv_stage_pos.append(float(string[6]))
                    current.append(float(string[-1][:-5]))
            except ValueError:
                raise ValueError("""Failed to convert a file name to a float. Make sure that index positions are correct for all files in file_names. 
                Also check separators""")
        except IndexError:
            raise ValueError(
                "Invalid index values. Make sure the index values are within the range of the file name strings.")

        ir_stage_pos = np.array(ir_stage_pos)
        uv_stage_pos = np.array(uv_stage_pos)
        file_order = np.array(file_order)
        current = np.array(current)
        counts = _get_counts(data_array)

        if sort == True:
            temp_idx = _sort_files_multistage(file_order, ir_stage_pos, uv_stage_pos)
            data_array = data_array[temp_idx]
            ir_stage_pos = ir_stage_pos[temp_idx]
            uv_stage_pos = uv_stage_pos[temp_idx]
            file_order = file_order[temp_idx]
            current = current[temp_idx]
            counts = counts[temp_idx]
        return data_array, ir_stage_pos, uv_stage_pos, file_order, counts, current

    if multistage == False:
        try:
            stage_positions = []
            file_order = []
            current = []
            try:
                for file in file_names:
                    string = list(map(str, file.split("/")))
                    string = list(map(str, string[-1].split("_")))
                    file_number = int(string[1])
                    file_order.append(file_number)
                    stage_positions.append(float(string[4]))
                    current.append(float(string[-1][:-5]))
            except ValueError:
                raise ValueError("""Failed to convert a file name to a float. Make sure that index positions are correct for all files in file_names. 
                Also check separators""")
        except IndexError:
            raise ValueError(
                "Invalid index values. Make sure the index values are within the range of the file name strings.")

        stage_positions = np.array(stage_positions)
        file_order = np.array(file_order)
        current = np.array(current)
        counts = _get_counts(data_array)

        if sort == True:
            temp_idx = _sort_files(file_order, stage_positions)
            data_array = data_array[temp_idx]
            stage_positions = stage_positions[temp_idx]
            file_order = file_order[temp_idx]
            current = current[temp_idx]
            counts = counts[temp_idx]

    return data_array, stage_positions, file_order, counts, current


def _sort_files_multistage(file_order, ir_stage_pos, uv_stage_pos):
    """ Hidden function for sorting files for experiments with multiple stage positions"""
    uni_stage_ir = np.unique(ir_stage_pos)  # Pump-probe stage position
    uni_stage_uv = np.unique(uv_stage_pos)

    if len(uni_stage_ir) > 1:
        stage_positions = ir_stage_pos
        print("sorting based on IR stage position")
    elif len(uni_stage_uv) > 1:
        stage_positions = uv_stage_pos
        print("sorting based on UV stage position")
    else:
        print("Bad Stage Positions")
    idx_list = []
    uni_stage = np.unique(stage_positions)
    for i in range(len(uni_stage)):
        # file_numbers = file_order[np.where(stage_positions==uni_stage[i])[0]];
        # file_numbers = file_numbers[idx_temp]
        stage_idx = np.where(stage_positions == uni_stage[i])[0]
        file_numbers = file_order[stage_idx]
        idx_temp = np.argsort(file_numbers)
        # print(file_numbers[idx_temp])
        idx_list.append(stage_idx[idx_temp])
    idx_list = np.array(idx_list)
    idx_list = np.reshape(idx_list, len(stage_positions))
    return idx_list


def _sort_files(file_order, stage_positions):
    """Hidden function for sorting files based on image name"""
    uni_stage = np.unique(stage_positions)  # Pump-probe stage position

    idx_list = []

    for i in range(len(uni_stage)):
        # file_numbers = file_order[np.where(stage_positions==uni_stage[i])[0]];
        # file_numbers = file_numbers[idx_temp]
        stage_idx = np.where(stage_positions == uni_stage[i])[0]
        file_numbers = file_order[stage_idx]
        idx_temp = np.argsort(file_numbers)
        # print(file_numbers[idx_temp])
        idx_list.append(stage_idx[idx_temp])
    idx_list = np.array(idx_list)
    idx_list = np.reshape(idx_list, len(stage_positions))
    return idx_list


### Cleaning Functions 

def remove_counts(data_array, stage_positions, file_order, counts, added_range = [], std_factor=STD_FACTOR, plot=False):
    # todo add edge option
    """
    Filters input parameters by removing any data where the total counts falls outside of the set filter. Default
    value is set to 3 standard deviations from the mean. Returns the same variables as it inputs but with
    different dimensions.

    ARGUMENTS:

    data_array (ndarray): 
        Multidimensional array of N x 1024 x 1024 where N is the length of file_names list
    stage_pos (array): 
        One dimensional array of length N containing the stage positions associated with each image.
    file_order (array): 
        One dimensional array of length N that reflects the order with which the images are taken.
    counts(ndarray): 
        One dimensional array of length N containing the total counts after summing over each array
        element.

    OPTIONAL ARGUMENTS:

    std_factor (int): 
        Default value is 3. Refers to cut off based on number of standard deviations from the mean.
    plot (boolean): 
        Default is False. Returns a plot of new and old counts.

    RETURNS:

    Returns same variables which it received as arguments with new N value.

    """

    init_length = len(counts)
    # Decide to use threshold or selected images
    counts_mean = np.mean(counts)  # Mean values of Total Counts of all images
    counts_std = np.std(counts)  # the STD of all the tc for all the iamges

    tc_good = np.squeeze(
        np.where(abs(counts - counts_mean) < std_factor * counts_std))  # Find out the indices of the low counts images
    new_array = data_array[tc_good]
    new_stage_positions = stage_positions[tc_good]
    new_counts = counts[tc_good]
    new_file_order = file_order[tc_good]

    for rng in added_range:
        new_array = np.concatenate((new_array[:rng[0]], new_array[rng[1]:]))
        print(len(new_array))
        new_stage_positions = np.concatenate((new_stage_positions[:rng[0]], new_stage_positions[rng[1]:]))
        new_counts = np.concatenate((new_counts[:rng[0]], new_counts[rng[1]:]))
        new_file_order = np.concatenate((new_file_order[:rng[0]], new_file_order[rng[1]:]))

    print(init_length - len(new_counts), " number of files removed from ", init_length, " initial files")

    if plot == True:
        plt.figure(figsize=(12, 4))  # Plot counts rate, images number at each posi, and bad images

        plt.plot(new_counts, '-d')
        plt.axhline(y=counts_mean, color='k', linestyle='-', linewidth=1, label="mean counts");
        plt.axhline(y=counts_mean - (3 * counts_std), color='r', linestyle='-', linewidth=0.5, label="min counts");
        plt.axhline(y=counts_mean + (3 * counts_std), color='r', linestyle='-', linewidth=0.5, label="max counts");
        plt.xlabel('Images orderd in lab time');
        plt.ylabel('Counts');
        plt.legend()
        plt.title('Total counts');

        plt.tight_layout()
        plt.show()

    return new_array, new_stage_positions, new_file_order, new_counts


def remove_background(data_array, remove_noise=True, plot=False, print_status=True):  
    """
    Takes in a 3d data array and calculates the means of the corners then linearly interpolates values based on corners across 3d array to 
    generate of background noise values using pandas.DataFrame.interpolate.

    ARGUMENTS:

    data_array (3d ndarray): 
        array containing all data

    OPTIONAL ARGUMENTS:

    remove_noise (boolean): 
        Default set to true, returns image with background subtracted. If false, returns
        interpolated background.
    plot (boolean): 
        Default set to False. Plots images showing the original image, interpolated background, and
        background subtracted image.
    print_status (boolean): 
        Default set to True. Prints a status update every nth image (n defined via CHECK_NUMBER).

    GLOBAL VARIABLES:

    CORNER_RADIUS (int): 
        defines the size of the corners being used in background suptraction.
    CHECK_NUMBER (int): 
        defines how often updates are given when print_status == True

    RETURNS:

    clean_data (3d ndarray): 
        Returns array of images with background subtracted if remove_noise == True, else returns
        array of interpolated background.

    """

    if not isinstance(data_array, np.ndarray):
        raise ValueError("Input data_array must be a numpy array.")
    if not isinstance(CORNER_RADIUS, int) and CORNER_RADIUS > 0:
        raise ValueError("bkg_range must be an integer > 0.")
    if not isinstance(remove_noise, bool):
        raise ValueError("remove_noise must be a boolean.")
    if not (2 * CORNER_RADIUS < len(data_array[:, 0, :]) and
            2 * CORNER_RADIUS < len(data_array[:, :, 0])):
        raise ValueError("2 * bkg-range must be less than both the number of rows and the number of columns.")

    clean_data = []
    bkg_data = []
    for i, image in enumerate(data_array):
        empty_array = np.empty(np.shape(image))
        empty_array = (ma.masked_array(empty_array, mask=True))
        empty_array[0, 0] = np.mean(image[0:CORNER_RADIUS, 0:CORNER_RADIUS])
        empty_array[-1, 0] = np.mean(image[-CORNER_RADIUS:, 0:CORNER_RADIUS])
        empty_array[0, -1] = np.mean(image[0:CORNER_RADIUS, -CORNER_RADIUS:])
        empty_array[-1, -1] = np.mean(image[-CORNER_RADIUS:, -CORNER_RADIUS:])
        empty_array = pd.DataFrame(empty_array).interpolate(axis=0)
        empty_array = pd.DataFrame(empty_array).interpolate(axis=1)
        bkg = pd.DataFrame.to_numpy(empty_array)
        bkg_data.append(bkg)
        clean_data.append(image - bkg)
        if print_status == True:
            if i % CHECK_NUMBER == 0:
                print(f"Subtracting background of {i}th image.")

    if plot == True:
        plt.figure()

        plt.subplot(1, 3, 1)
        plt.imshow(data_array[0])
        plt.title("Original Data")

        plt.subplot(1, 3, 2)
        plt.imshow(bkg_data[0])
        plt.title("Interpolated Background")

        plt.subplot(1, 3, 3)
        plt.imshow(clean_data[0])
        plt.title("Background Free Data")
        plt.tight_layout()
        plt.show()

    if remove_noise == True:
        return np.array(clean_data)
    else:
        return np.array(bkg_data)


def _remove_background(image):
    """
    Takes in a 2d data array (using the mean array is recommended) and calculates the means of the corners. Linearly
    interpolates values across 2d array to generate of background noise values using pandas.DataFrame.interpolate.

    ARGUMENTS:

    image (2d ndarray): 
        single image array

    GLOBAL VARIABLES:

    CORNER_RADIUS (int): 
        defines the size of the corners being used in background suptraction.
    CHECK_NUMBER (int): 
        defines how often updates are given when print_status == True

    RETURNS:

    clean_image (2d ndarray): 
        Returns image with background subtracted if remove_noise == True, else returns array of interpolated background.

    """
    if not isinstance(image, np.ndarray):
        raise ValueError("Input data_array must be a numpy array.")
    if not isinstance(CORNER_RADIUS, int) and CORNER_RADIUS > 0:
        raise ValueError("bkg_range must be an integer > 0.")
    if not (2 * CORNER_RADIUS < len(image[0, :]) and
            2 * CORNER_RADIUS < len(image[:, 0])):
        raise ValueError("2 * bkg-range must be less than both the number of rows and the number of columns.")

    clean_data = []
    bkg_data = []

    empty_array = np.empty(np.shape(image))
    empty_array = (ma.masked_array(empty_array, mask=True))
    empty_array[0, 0] = np.mean(image[0:CORNER_RADIUS, 0:CORNER_RADIUS])
    empty_array[-1, 0] = np.mean(image[-CORNER_RADIUS:, 0:CORNER_RADIUS])
    empty_array[0, -1] = np.mean(image[0:CORNER_RADIUS, -CORNER_RADIUS:])
    empty_array[-1, -1] = np.mean(image[-CORNER_RADIUS:, -CORNER_RADIUS:])
    empty_array = pd.DataFrame(empty_array).interpolate(axis=0)
    empty_array = pd.DataFrame(empty_array).interpolate(axis=1)
    bkg = pd.DataFrame.to_numpy(empty_array)
    bkg_data.append(bkg)
    clean_data.append(image - bkg)

    return np.squeeze(np.array(clean_data)), np.squeeze(np.array(bkg_data))


def remove_background_pool(data_array, remove_noise=True, plot=False):
    """ 
    Removes the background of images based on the corners. Runs the hidden function _remove_background and runs it in parallel.

    ARGUMENTS:

    data_array (3d array): 
        data array of all images

    OPTIONAL ARGUMENTS:

    remove_noise (boolean): 
        Default set to true. Returns data array with noise removed. If false, only returns the interpolated background
    plot (boolean): 
        Default set to false. When true, plots an example of original data, interpolated background, and cleaned image.

    RETURNS:

    clean_data (3d array): 
        Original data with background removed when remove_noise==True
    or
    backgrounds (3d array):
        Interpolated background for each image when remove_noise==False

    """
    clean_data = []
    backgrounds = []
    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
        results = executor.map(_remove_background, data_array)

    for result in results:
        clean, bkg = result
        clean_data.append(clean)
        backgrounds.append(bkg)

    clean_data = np.array(clean_data)
    backgrounds = np.array(backgrounds)

    if plot == True:
        fig = plt.figure(figsize = (12,4)) # Plot counts rate, images number at each posi, and bad images
            
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        vmin_1_1, vmax_1_1 = 10**-0, 4000 

        #Before
        img1 = axes[0].imshow(data_array[0], cmap='plasma', norm=matplotlib.colors.LogNorm(vmin=vmin_1_1, vmax=vmax_1_1))
        axes[0].set_title('Original Data with Outliers Removed')
        axes[0].set_xlabel('X-position')
        axes[0].set_ylabel('Y-position')
        colorbar1 = fig.colorbar(img1, ax=axes[0])  # Add colorbar to the first subplot

        #Noise
        img2 = axes[1].imshow(backgrounds[0], cmap='plasma',  norm=matplotlib.colors.LogNorm(vmin=vmin_1_1, vmax=vmax_1_1))  
        # Use the same normalization as img1
        axes[1].set_title('Interpolated Noise Data')
        axes[1].set_xlabel('X-position')
        axes[1].set_ylabel('Y-position')
        colorbar2 = fig.colorbar(img2, ax=axes[1])  # Add colorbar to the second subplot

        # Noise Removed
        img3 = axes[2].imshow(clean_data[0], cmap='plasma',  norm=matplotlib.colors.LogNorm(vmin=vmin_1_1, vmax=vmax_1_1))  # Logarithmic scaling
        axes[2].set_title('After Background Removal')
        axes[2].set_xlabel('X-position')
        axes[2].set_ylabel('Y-position')
        colorbar3 = fig.colorbar(img3, ax=axes[2])  # Add colorbar to the third subplot
    if remove_noise == True:
        return clean_data
    else:
        return backgrounds


def _remove_xrays(mean_data, std_data, image, std_factor=STD_FACTOR):
    """This is the hidden function that is run within the remove_xrays_pool function.

    ARGUMENTS:

    mean_data (2d array): 
        average image of all data in data_array from parent function.
    std_data (2d array): 
        image with standard deviation values from all data in data_array in parent function.
    image (2d array): 
        array of image like data with length N where N is number of images.

    OPTIONAL ARGUMENTS:

    std_factor (int): 
        Default set to 3. Defines the threshold for removing pixels with |pixel_value - mean| > std_factor*std

    RETURNS:

    clean_data (2d array): 
        array of image like data with shape of input data array where errant pixels are now masked based on the set threshold
        amt_rmv (int): count of all pixels removed per image

    """

    upper_threshold = mean_data + std_factor * std_data
    clean_data = ma.masked_greater_equal(image, upper_threshold)
    amt_rmv = np.sum(clean_data.mask)
    return clean_data, amt_rmv


def remove_xrays(data_array, plot=True): # testing for timing

    """
    Filters out any pixels that are more than set threshold value based on the standard deviation of the
    average pixel value by running the hidden function _remove_xrays in parallel.

    ARGUMENTS:

    data_array (3d array): 
        array of image like data with length N where N is number of images.

    OPTIONAL ARGUMENTS:

    plot (boolean): 
        Default set to True. Plots the percentage of pixeled removed during cleaning process
    std_factor (int): 
        Default set to 3. Defines the threshold for removing pixels with |pixel_value - mean| > std_factor*std

    RETURNS:

    clean_data (3d array): 
        array of image like data with shape of input data array where errant pixels are now masked based on the set threshold

    """

    mean_data = np.mean(data_array, axis=0)
    std_data = np.std(data_array, axis=0)
    print("Removing hot pixels from all data")

    clean_data = []
    amt_rmv = []

    for i in range(len(data_array)):
        clean, amt = _remove_xrays(data_array[i], mean_data, std_data)
        clean_data.append(clean)
        amt_rmv.append(amt)

    pct_rmv = np.array(amt_rmv) / (len(data_array[1]) * len(data_array[2])) * 100

    if plot == True:
        plt.figure()
        plt.subplot(1, 3, 1)
        plt.plot(pct_rmv)
        plt.title("Percent Pixels Removed")
        plt.xlabel("Image Number")
        plt.ylabel("Percent")

        plt.subplot(1, 3, 2)
        plt.imshow(data_array[0])
        plt.title("Original Image")

        plt.subplot(1, 3, 3)
        plt.imshow(clean_data[0])
        plt.title("Cleaned Image")
        plt.tight_layout()
        plt.show()

    return clean_data


def remove_xrays_pool(data_array, plot=True, std_factor=STD_FACTOR):
    """
    Filters out any pixels that are more than set threshold value based on the standard deviation of the
    average pixel value by running the hidden function _remove_xrays in parallel.

    ARGUMENTS:

    data_array (3d array): 
        array of image like data with length N where N is number of images.

    OPTIONAL ARGUMENTS:

    plot (boolean): 
        Default set to True. Plots the percentage of pixeled removed during cleaning process
    std_factor (int): 
        Default set to 3. Defines the threshold for removing pixels with |pixel_value - mean| > std_factor*std

    RETURNS:

    clean_data (3d array): 
        array of image like data with shape of input data array where errant pixels are now masked based on the set threshold

    """

    mean_data = np.mean(data_array, axis=0)
    std_data = np.std(data_array, axis=0)
    print("Removing hot pixels from all data")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
        futures = [executor.submit(partial(_remove_xrays, mean_data, std_data), data) for
                   data in data_array]
        results = [future.result() for future in futures]

    clean_data = []
    amt_rmv = []
    for result in results:
        data, amt = result
        clean_data.append(data)
        amt_rmv.append(amt)

    pct_rmv = np.array(amt_rmv) / (len(data_array[1]) * len(data_array[2])) * 100

    if plot == True:
        plt.figure()
        plt.subplot(1, 3, 1)
        plt.plot(pct_rmv)
        plt.title("Percent Pixels Removed")
        plt.xlabel("Image Number")
        plt.ylabel("Percent")

        plt.subplot(1, 3, 2)
        plt.imshow(data_array[0])
        plt.title("Original Image")

        plt.subplot(1, 3, 3)
        plt.imshow(clean_data[0])
        plt.title("Cleaned Image")
        plt.tight_layout()
        plt.show()

    return np.array(clean_data)


def subtract_background(data_array, mean_background, plot=True):
    """Takes in 3d data_array and subtracts each image from the input mean_background 2d array. Returns cleaned
    data_array

    ARGUMENTS:

    data_array (3d array): 
        original data array
    mean_background (2d array): 
        average of background images

    OPTIONAL ARGUMENTS:

    plot (boolean): 
        Default is True. Plots example original image and background subtracted image

    RETURNS:

    clean_data (3d array): 
        data_array - mean_background
    """

    clean_data = data_array - mean_background

    if plot == True:
        plt.figure()
        plt.subplot(1, 2, 1)
        plt.imshow(data_array[0])
        plt.title("Original Image")

        plt.subplot(1, 2, 2)
        plt.imshow(clean_data[0])
        plt.title("Cleaned Image")
        plt.tight_layout()
        plt.show()

    return clean_data


# Masking and Center Finding Functions

def mask_generator_alg(image, fill_value=np.nan, add_rectangular=False, plot=False):
    """
    Generate mask to cover unwanted area

    ARGUMENTS:

    image : 2D array
        Diffraction pattern.

    OPTIONAL ARGUMENTS: 

    fill_value : int, float, or nan, optional
        Value that use to fill the area of the mask. The default is np.nan.
    add_rectangular : boolean, optional
        Additional mask with rectangular shape. The default is True.
    showingfigure : boolean, optional
        Show figure of the result of applied masks. The default is False.

    GLOBAL VARIABLES:

    MASK_CENTER (1d array, tuple, or list that contains only two values):
        Center for generating mask cover unscattered electron beam.
    MASK_RADIUS (int):
        Radius of the mask.
    ADDDED_MASK (list of 3-value-lists):
        Additional masks. Input gonna be [[x-center, y-center, radius], [...], ...] The default is [].


    RETURNS:
    
    mask (binary 2D array):
        Result of all the masks in an image.

    """

    mask = np.ones(image.shape)
    rows, cols = draw.disk((MASK_CENTER[1], MASK_CENTER[0]), MASK_RADIUS, shape=mask.shape)
    mask[rows, cols] = fill_value

    if len(ADDED_MASK) == 0:
        pass
    else:
        for i in ADDED_MASK:
            rows, cols = draw.disk((i[1], i[0]), i[2], shape=mask.shape)
            mask[rows, cols] = fill_value

    # retangular mask
    if add_rectangular == True:
        rr, cc = draw.rectangle((0, 590), extent=(500, 40), shape=image.shape)  # (0,535) for iodobenzene
        mask[rr, cc] = fill_value
        # 515

    if plot == True:
        fig, axs = plt.subplots(1, 3, figsize=(15, 5))

        # First subplot: Mean of Unmasked Data Array
        axs[0].imshow(image)
        axs[0].set_title("Mean of Unmasked Data Array")
        cbar = plt.colorbar(axs[0].imshow(image), ax=axs[0])
        cbar.ax.set_ylabel('Intensity')

        masked_data = mask*image
        # Second subplot: Mean of Masked Data Array
        axs[1].imshow(masked_data)
        axs[1].set_title("Mean of Masked Data Array")
        cbar = plt.colorbar(axs[1].imshow(masked_data), ax=axs[1])
        cbar.ax.set_ylabel('Intensity')

        # Third subplot: Contour map of average data
        x = np.arange(300, 700)
        y = np.arange(300, 700)
        X, Y = np.meshgrid(y, x)
        pc = axs[2].pcolormesh(x, y, np.log(masked_data[300:700, 300:700]), shading='auto')
        cs = axs[2].contour(X, Y, np.log(masked_data[300:700, 300:700]), levels=20, colors='w')
        axs[2].set_title('Contour map of average data')
        cbar = fig.colorbar(pc, ax=axs[2])
        cbar.ax.set_ylabel('Log(Intensity)')

        # Adjust layout to prevent overlap
        plt.tight_layout()

        # Show the combined figure
        plt.show()

    return mask


def apply_mask(data_array, fill_value=np.nan, add_rectangular=False,
               plot=False):  # todo change mask parameters to global variables
    """ Applies a mask to individual images in the data array.

    ARGUMENTS:

    data_array_1d : 2D array
        Diffraction pattern.

    OPTIONAL ARUGMENTS:

    fill_value : int, float, or nan, optional
        Value that use to fill the area of the mask. The default is np.nan.
    add_rectangular : boolean, optional
        Additional mask with rectangular shape. The default is True.
    showingfigure : boolean, optional
        Show figure of the result of applied masks. The default is False.

    GLOBAL VARIABLES:

    MASK_CENTER : 1D array, tuple, or list that contains only two values
        Center for generating mask cover unscattered electron beam.
    MASK_RADIUS : int
        Radius of the mask.
    ADDED_MASK : list of 3-value-lists, optional
        Additional masks. Input gonna be [[x-center, y-center, radius], [...], ...] The default is [].

    RETURNS:

    mask : binary 2D array
        Result of all the masks in an image.

    """
    mean_data = np.nanmean(data_array, axis=0)
    masked_data = data_array * mask_generator_alg(mean_data, fill_value, add_rectangular)
    masked_mean = np.nanmean(masked_data, axis=0)

    if plot == True:
        fig, axs = plt.subplots(1, 3, figsize=(15, 5))

        # First subplot: Mean of Unmasked Data Array
        axs[0].imshow(mean_data)
        axs[0].set_title("Mean of Unmasked Data Array")
        cbar = plt.colorbar(axs[0].imshow(mean_data), ax=axs[0])
        cbar.ax.set_ylabel('Intensity')

        # Second subplot: Mean of Masked Data Array
        axs[1].imshow(masked_mean)
        axs[1].set_title("Mean of Masked Data Array")
        cbar = plt.colorbar(axs[1].imshow(masked_mean), ax=axs[1])
        cbar.ax.set_ylabel('Intensity')

        # Third subplot: Contour map of average data
        x = np.arange(300, 700)
        y = np.arange(300, 700)
        X, Y = np.meshgrid(y, x)
        pc = axs[2].pcolormesh(x, y, np.log(masked_mean[300:700, 300:700]), shading='auto')
        cs = axs[2].contour(X, Y, np.log(masked_mean[300:700, 300:700]), levels=20, colors='w')
        axs[2].set_title('Contour map of average data')
        cbar = fig.colorbar(pc, ax=axs[2])
        cbar.ax.set_ylabel('Log(Intensity)')

        # Adjust layout to prevent overlap
        plt.tight_layout()

        # Show the combined figure
        plt.show()

    return masked_data


def finding_center_alg(image, plot=False, title='Reference Image', thresh_input=0):
    """
    Algorithm for finding the center of diffraction pattern

    ARGUMENTS:
    
    data_array : 2D array
        Diffraction pattern.

    OPTIONAL ARGUMENTS:

    plot : boolean, optional
        Show figure of the result of center finding. The default is False.
    title : str, optional
        Title of the figure. The default is 'Reference image'.


    GLOBAL VARIABLES:

    DISK_RADIUS : int, optional
        Generates a flat, disk-shaped footprint. The default is 3.
    CENTER_GUESS : tuple contains 2 values, optional
        Guessing center position to generate temporary mask. The default is (532, 520).
    RADIUS_GUESS : int, optional
        Guessing radius of the temporary mask. The default is 80.

    RETURNS
    
    center_x : int
        Center value on x axis.
    center_y : int
        Center value of y axis.
    radius : int
        Radius of ring used for finding center.

    """

    if thresh_input == 0:
        thresh = threshold_otsu(image)
    else:
        thresh = thresh_input

    cxt, cyt = [], []
    ## apply median filter to help
    image = ss.medfilt2d(image, kernel_size=9)
    for th in [1]:
        thresh *= th
        mask_temp = mask_generator_alg(image, fill_value=False,
                                       add_rectangular=False)
        mask_temp = util.invert(mask_temp.astype(bool))
        bw = closing(image > thresh, disk(
            DISK_RADIUS))  # Return grayscale morphological closing of an image. Square(): generate the footprint to close the gap between data points
        cleared = clear_border(bw + mask_temp)
        label_image = label(cleared)
        props = regionprops_table(label_image, properties=('centroid',
                                                           'axis_major_length',
                                                           'axis_minor_length'))
        dia = np.array([props['axis_major_length'], props['axis_minor_length']])
        dia = np.mean(dia, axis=0)
        radius = np.amax(dia) / 2
        idx = np.where(dia == np.amax(dia))[0][0]
        cxt.append(props['centroid-1'][idx])
        cyt.append(props['centroid-0'][idx])

    center_x = np.mean(cxt)
    center_y = np.mean(cyt)

    if plot == True:
        fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))
        ax1.imshow(image)
        ax2.imshow(label_image)
        ax3.imshow(bw)

        for cc in range(len(cxt)):
            circ = patches.Circle((cxt[cc], cyt[cc]), radius, linewidth=1, edgecolor="r", facecolor="none",
                                  linestyle='--')
            ax1.add_patch(circ)
            circ = patches.Circle((cxt[cc], cyt[cc]), radius, linewidth=2, edgecolor="r", facecolor="none")
            ax2.add_patch(circ)
            circ = patches.Circle((cxt[cc], cyt[cc]), radius, linewidth=2, edgecolor="r", facecolor="none")
            ax3.add_patch(circ)

        for ax in (ax1, ax2, ax3):
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        ax1.set_title(title, fontsize=20)
        ax2.set_title("Center [X = " + str(center_x) + ", Y = " + str(center_y) + "]", fontsize=20)
        ax3.set_title("Binary image", fontsize=20)

        ax1.axvline(center_x, linestyle='--', lw=1, color='tab:red')
        ax1.axhline(center_y, linestyle='--', lw=1, color='tab:red')

        ax2.axvline(center_x, linestyle='--', lw=2, color='tab:red')
        ax2.axhline(center_y, linestyle='--', lw=2, color='tab:red')

        plt.tight_layout()
        plt.show()

    return center_x, center_y, radius, thresh


def find_center_pool(data_array, plot=True, print_stats=True):
    """ Finds center of each image in the data array using concurrent.futures.ThreadPoolExecutor to quickly process
    many data files.

    ARGUMENTS:

    data_array (ndarray): 
        array of image like data with shape Nx1024x1024

    OPTIONAL ARGUMENTS:

    plot (boolean): 
        Default is set to True. When true, plots an image of the values for center_x and center_y with respect to pixel number
    print_stats (boolean): 
        Default is set to True. Prints the average value for center_x and center_y and prints the percent failure rate.

    GLOBAL VARIABLES:

    CENTER_GUESS (tuple): 
        initial guess for center position
    RADIUS_GUESS (int): 
        initial guess for the radius
    DISK_RADIUS (int): 
        value for disk radius used in mapping

    RETURNS:

    center_x (array):
        One-dimensional array of x values for the center position of each image
    center_y (array): 
        One-dimensional array of y values for the center position of each image"""

    center_x = []
    center_y = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
        results = executor.map(finding_center_alg, data_array)

    for result in results:
        cx, cy, _, _ = result
        center_x.append(cx)
        center_y.append(cy)

    center_x = np.array(center_x)
    center_y = np.array(center_y)
    if plot == True:
        plt.figure(figsize=(10, 10))
        plt.subplot(2, 1, 1)
        plt.plot(center_x[:])
        plt.title("X values for Centers")
        plt.xlabel("Image Number")
        plt.ylabel("Pixel Value")

        plt.subplot(2, 1, 2)
        plt.plot(center_y[:-3])
        plt.title("Y values for Centers")
        plt.xlabel("Image Number")
        plt.ylabel("Pixel Value")

        plt.show()
    if print_stats == True:
        x_ave = np.mean(center_x[np.where(center_x != CENTER_GUESS[0])[0]])
        y_ave = np.mean(center_y[np.where(center_y != CENTER_GUESS[1])[0]])
        center_x[np.where(center_x == CENTER_GUESS[0])[0]] = x_ave
        center_y[np.where(center_y == CENTER_GUESS[1])[0]] = y_ave
        center_ave = x_ave, y_ave
        print(r'Averaged ctr is ' + str(center_ave))
        fail_count = np.count_nonzero(np.array(center_x) == CENTER_GUESS[0])

        print(
            f"Percentage of images where the center finding failed (i.e., found the guess value): {fail_count / len(data_array) * 100}")
    return center_x, center_y


def find_beam_center(I, center=[500, 500], r=200, printr2='no', recursiontime=0):
    recursiontime += 1
    # up down right left,r away pixles average
    # fit_value=average([I[center[0]+r][center[1]],I[center[0]-r][center[1]],I[center[0]][center[1]+r],I[center[0]][center[1]-r]])
    fit_value = np.average([I[round(center[0]) + r][round(center[1])], I[round(center[0]) - r][round(center[1])],
                            I[round(center[0])][round(center[1]) + r], I[round(center[0])][round(center[1]) - r]])

    [X_f, Y_f] = np.where((I > 0.999 * fit_value) & (I < 1.001 * fit_value))

    a = len(X_f)
    i = 0
    # delete fit_points which are too far away from fit_circle, range from 0.5r to 1.5r
    while (i < a):
        ri2 = (X_f[i] - center[0]) ** 2 + (Y_f[i] - center[1]) ** 2
        if (ri2 > (1.5 * r) ** 2) or (ri2 < (0.5 * r) ** 2):
            X_f = np.delete(X_f, i)
            Y_f = np.delete(Y_f, i)
            i -= 1
            a -= 1
        i += 1

    center_new, r_new = fit_circle([X_f, Y_f], printr2)

    if r_new == 0:
        return [0, 0]
    elif ((center[0] - center_new[0]) ** 2 + (center[1] - center_new[1]) ** 2) <= 1:
        # new center pretty close to old center
        return center_new
    elif recursiontime >= 10:
        return [0, 0]
    else:
        # else: iterate
        return find_beam_center(I, center_new, r_new, recursiontime=recursiontime)


def fit_circle(fit_points, printr2='yes'):
    # circle function: ax+by+c=-(x^2+y^2)

    A = np.empty((len(fit_points[0]), 3))  # Find center for 3 thimes
    B = np.empty(len(fit_points[0]))

    for i in range(len(fit_points[0])):
        B[i] = -(fit_points[0][i] ** 2 + fit_points[1][i] ** 2)
        A[i][0] = fit_points[0][i]
        A[i][1] = fit_points[1][i]
        A[i][2] = 1

    # A[i]=[xi,yi,1], B[i]=-(xi^2+yi^2), par=[a,b,c]
    # namely A*par=B, least square method
    if np.linalg.det(np.dot(A.T, A)) == 0:
        return [], 0
    par = np.dot(np.dot(np.linalg.inv(np.dot(A.T, A)), A.T), B)

    # correlation coeff, if not very close to 1(less than 3 nines), be careful
    if printr2 == 'yes':
        y_ave = np.mean(B)
        r2 = sum((np.dot(A, par) - y_ave) ** 2) / sum((B - y_ave) ** 2)
        print(r2)

    center_new = [(-par[0] / 2), (-par[1] / 2)]  # no-Round the center, not working for the moment
    r_new = round(
        np.sqrt(par[0] ** 2 + par[1] ** 2 - 4 * par[2]) / 2)  # no-round the r range, not working for the moment
    # print('ct found:'+str(center_new))

    # center_new=[round(-par[0]/2),round(-par[1]/2)] # Round the center
    # r_new=round(sqrt(par[0]**2+par[1]**2-4*par[2])/2) # round the r range

    return center_new, r_new


def _median_filter(image, kernel_size = 5):
    """
    Applies the scipy.ndimage.median_filter function to the image then returns the filtered image"""

    #corners = (np.median(data_array_1d[-50:, -50:]), np.median(data_array_1d[-50:, :50]), np.median(data_array_1d[:50, -50:]), 
                    #np.median(data_array_1d[:50, :50]))
    #floor = float(np.mean(corners))
    from scipy.ndimage import median_filter
    filt_data = median_filter(image, kernel_size)

    return filt_data


def median_filter_pool(data_array, plot=True):
    """Takes in a large 3D array of data and applies the scipy.ndimage.median_filter on them in parallel processing using the hidden function
    _median_filter.

    ARGUMENTS: 

    data_array (3d array):
        array of all images
    
    OPTIONAL ARGUMENTS:

    plot(boolean): Default set to True
        When true, plots an example of the original and of the filtered image

    RETURNS: 
        
    filtered_data (3d array):
        filtered data array of the same size as the input array"""
    
    filtered_data = []
    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
        results = executor.map(_median_filter, data_array)
        
    for result in results:
        filtered_data.append(result)
    
    filtered_data = np.array(filtered_data)

    if plot == True:
        plt.figure()
        plt.subplot(1,2,1)
        plt.imshow(data_array[0])
        plt.title("Original Image")
        
        plt.subplot(1,2,2)
        plt.imshow(filtered_data[0])
        plt.title("Filtered Image")
        plt.show()

    return filtered_data

### Azimuthal Averaging and Radial Outlier Removal Functions
# todo: clean and optimize
def cart2pol(x, y): 
    r = np.sqrt(x**2 + y**2)
    theta = np.arctan2(y, x)
    return r, theta


def preprocess_for_azimuthal_checking(center, dat):
    w, h = dat.shape
    xmat, ymat = np.meshgrid(np.arange(0,w,1)-center[0],np.arange(0,h,1)-center[1])
    rmat, _ = cart2pol(xmat, ymat)
    rmat = np.around(rmat)
    dat = dat.astype(float)
    xlength = int(np.amax([np.amax(abs(xmat)),np.amax(abs(ymat))]))
    
    return xlength, rmat


def cleaning_2d_data(center, dat, std_factor=STD_FACTOR):
    xlength, rmat = preprocess_for_azimuthal_checking(center, dat)
    res2d = np.copy(dat)
    
    mask_detect = True
    for i in range(xlength):
        roi = np.copy(dat[rmat==int(i+1)])
        if len(roi)==0:
            break
        if int(i+1)>=500:
            break
        # Check the area of mask so the azimuthal integration will ignore that.
        if mask_detect==True:
            if np.sum(np.isnan(roi)) < len(roi):
                mask_detect=False
                
        if mask_detect==False:
            # remove value that higher or lower than correct_factor*standard deviation
            roi = outlier_rev_algo(roi, std_factor=std_factor)
        
        res2d[rmat==int(i+1)] = np.copy(roi)
    return res2d


def outlier_rev_algo(dat1d, std_factor=STD_FACTOR, fill_value = 'nan'):
    index = np.logical_or(dat1d>=np.nanmean(dat1d)+std_factor*np.nanstd(dat1d), dat1d<=np.nanmean(dat1d)-std_factor*np.nanstd(dat1d))
    if fill_value == 'nan':
        dat1d[index] = np.nan
    elif fill_value == 'average':
        dat1d[index] = np.nanmean(dat1d)
    return dat1d


def remove_radial_outliers_pool(data_array, center, plot=False):
    """
    Removes instances of outlier pixels based on the radial average of the image. Runs the hidden function _remove_radial_outliers in parallel. 
    Works by first converting an individual array to polar coordinates and remaps to create an average image. Then performs a logical check on 
    the original image compared to interpolated image. 
    
    ARGUMENTS: 
    
    data_array (3d array):
        Original data 
    center (list):
        Can either be an average center value of form [x, y] or a list of centers of form [[x1,y1], [x2, y2], ...]

    OPTIONAL ARGUMENTS: 

    plot (boolean):
        default set to False. When true, plots an example of original data, the interpolated average image, and the cleaned image

    RETURNS:

    clean_data (3d array):
        data with outliers removed

    """

    clean_data = []
    rmv_count = []

    if len(center) > 2:
        print("Using all center values ")
        print("Removing radial outliers from all data")
        with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
            # Zip the arrays together and submit to the executor
            results = list(executor.map(lambda args: cleaning_2d_data(*args), zip(center, data_array)))
        for result in results:
            clean_image = result
            clean_data.append(clean_image)

    elif len(center) == 2:
        print("Using average center")
        print("Removing radial outliers from all data")
        with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
            futures = [executor.submit(partial(cleaning_2d_data, center), data) for data in data_array]
            results = [future.result() for future in futures]

        for result in results:
            clean_image = result
            clean_data.append(clean_image)
            rmv = np.isnan(clean_image)
            rmv_count.append(np.sum(rmv)/(1024*1024))

    clean_data = np.array(clean_data)
    rmv_count = np.array(rmv_count)

    if plot == True:
        plt.figure()
        plt.subplot(1, 3, 1)
        plt.imshow(data_array[0])
        # plt.xlim(300, 600)
        # plt.ylim(300, 600)
        plt.title("Original Image")

        plt.subplot(1, 3, 2)
        plt.imshow(clean_data[0])
        # plt.xlim(300, 600)
        # plt.ylim(300, 600)
        plt.title("Cleaned Image")

        plt.subplot(1, 3, 3)
        plt.plot(rmv_count)
        plt.title("Percent of nan Values per Image")
        plt.tight_layout()
        plt.show()

    return clean_data


def _azimuthal_average(center, image, normalize=True):
    """
    TEMPORARY CODE FOR FINDING AZIMUTHAL AVERAGE

    BETA VERSION, NOT DEVELOPED 
    """
    # Create meshgrid of coordinates
    x, y = np.indices(image.shape[:2])

    # Convert coordinates to polar coordinates
    theta = np.arctan2(y - center[1], x - center[0])
    rho = np.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2)

    # Normalize rho to [0, max_radius] for image indexing
    max_radius = np.sqrt(center[0] ** 2 + center[1] ** 2)
    print(f"Max radius = {max_radius}")
    #max_radius = len(image[0]) - min(center)

    rho_normalized = (rho / max_radius) * (image.shape[0] / 2)

    # Convert theta to degrees and ensure it's within [0, 360]
    theta_degrees = np.degrees(theta) % 360

    # Create polar image with correct dimensions
    polar_image = np.empty((int(max_radius), 360))
    polar_image.fill(np.nan)

    # Fill in the polar image with values from the original image
    for i in range(image.shape[0]):
        for j in range(image.shape[1]):
            polar_image[int(rho_normalized[i, j]), int(theta_degrees[i, j])] = image[i, j]

    azi_ave = np.nanmean(polar_image, axis=1)
    azi_std = np.nanstd(polar_image, axis=1)

    return azi_ave, azi_std


def get_azimuthal_average_pool(data_array, center, normalize=False, plot=False):
    """
    TEMPORARY CODE FOR FINDING AZIMUTHAL AVERAGE

    BETA VERSION, NOT DEVELOPED 
    """
    average_data = []
    std_data = []

    if len(center) > 2:
        print("Using all center values ")
        print("Calculating azimuthal average for all data")
        with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
            # Zip the arrays together and submit to the executor
            results = list(executor.map(lambda args: _azimuthal_average(*args), zip(data_array, center)))
        for result in results:
            ave, std = result
            average_data.append(ave)
            std_data.append(std)

    elif len(center) == 2:
        print("Using average center")
        print("Calculating azimuthal average for all data")
        with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_PROCESSORS) as executor:
            futures = [executor.submit(partial(_azimuthal_average, center), data) for data in data_array]
            results = [future.result() for future in futures]

        for result in results:
            ave, std = result
            average_data.append(ave)
            std_data.append(std)

    average_data = np.array(average_data)
    std_data = np.array(std_data)

    if normalize == True:
        norm_data = normalize_to_baseline(average_data)
        average_data = norm_data


    if plot == True:
        plt.figure()
        plt.plot(average_data[0])
        plt.title("Example of Azimuthal Average")
        plt.show()

    return average_data, std_data


def normalize_to_baseline(data_array2d, min_val=50, max_val=100):
    """
    Normalizes a 2d data set based on the average of the data between the min_val and the max_val

    ARGUMENTS:

    data_array2d (2d array):
        azimuthally averaged 2d data with shape of length unique stage positons vs maximum s distance
    
    OPTIONAL ARGUMENTS:

    min_val (int): 
        Default set to 50. Defines the minimum point for normalization range
    max_val (int): 
        Default set to 100. Defines the maximum point for normalization range

    RESULTS:

    data_norm (2d array):
        normalized array of azimuthally averaged data with same shape as data_array2d
            
    """

    data_array2d[:, :25] = np.nan
    data_mean = np.nanmean(data_array2d, axis=0)
    norm_factor = np.nansum(data_mean[min_val:max_val])
    data_norm = []
    for i in range(len(data_array2d)):
        offset = np.nansum(data_array2d[i, min_val:max_val])
        norm = data_array2d[i] * (norm_factor / offset)
        data_norm.append(norm)

    data_norm = np.array(data_norm)

    return data_norm


def poly_fit(data_array, x_vals, degree = 2, plot=True, return_baseline=False):
    """
    Calculates a polynomial fit of the data_array with respect to the x_vals. 

    ARGUMENTS:

    data_array (1d or 2d array):
        1d or 2d data array to be fit, normally used on the dI/I or dI values after azimuthal averaging. Code checks the shape of the array
    x_vals (1d array):
        list of x values related to the data array (i.e., s values)

    OPTIONAL ARGUMENTS:

    degree (int):
        default set to True. Defines the degree of the polynomial used for fitting
    return_baseline (boolean):
        default set to False. When true, returns both the corrected data and the calculated baseline
    
    RESULTS:
    
    corrected_data (2d array):
        input 2d array - calculated baselines
    baselines (2d array):
        calculated baseline for each data set in the array. Only returned when return_baseline == True
    
    """

    if len(data_array.shape) == 2:
        baseline2d = []
        for i in range(len(data_array)):
            temp_data = np.copy(data_array[i])
            idx_nan = ~np.isnan(temp_data)
            coeff = np.polyfit(x_vals[idx_nan],temp_data[idx_nan], degree)
            baseline = np.polyval(coeff,x_vals)
            baseline2d.append(baseline)

        baseline2d = np.array(baseline2d)
        corrected_data = data_array - baseline2d
        
    elif len(data_array.shape) == 1:
        temp_data = data_array
        idx_nan = ~ np.isnan(temp_data)
        coeff = np.polyfit(x_vals[idx_nan], temp_data[idx_nan], degree)
        baseline2d = np.polyval(coeff, x_vals)
        
        corrected_data = data_array - baseline2d
    else:
        print("Data Array must be 1D or 2D array")

    if plot == True:
        plt.figure()
        plt.subplot(1,2,1)
        plt.plot(data_array[0])
        plt.plot(baseline2d[0])
        plt.xlabel("pixel")
        plt.title("delta I/I original with fit line")

        plt.subplot(1,2,2)
        plt.plot(corrected_data[0])
        plt.xlabel("pixel")
        plt.title("delta I/I corrected")

        plt.tight_layout()
        plt.show()

    if return_baseline == True:
        return corrected_data, baseline2d
    else:
        return corrected_data


# Saving and Loading Data

def save_data(file_name, group_name, run_number, azimuthal_data, stage_positions):
    """
    Saves the azimuthal average and stage positions after processing to an h5 file with the specified file_name. The group name specifies the 
    group subset the data relates to and the run number tags the number. For example, when running large data sets, each run will be a subset
    of data that was processed. If you have multiple experiments that can be grouped, you can save them with different group names to the same 
    h5 file. The saved data is used for further analysis. 

    ARGUMENTS:

    file_name (str):
        unique file name for the data to be saved. Can specify a full path. 
    group_name (str):
        label for the group of data that is being processed
    run_number (int):
        specifies ths subset of data being processed
    azimuthal_data (2d array):
        azimuthally averaged scattering intensity of run subset
    stage_positions (1d array):
        stage positions relating to the scattering intensity for the run subset

    """

    # Open the HDF5 file in append mode
    with h5py.File(file_name, 'a') as f:
        # Check if the group exists, create it if not
        if group_name not in f:
            group = f.create_group(group_name)
            print(f"Creating new group called {group_name}")
        else:
            group = f[group_name]
        
        # Create unique dataset names for each run
        dataset_name_var1 = f'I_run_{run_number}'
        dataset_name_var2 = f'stage_positions_run_{run_number}'
        
        # Save the datasets
        group.create_dataset(dataset_name_var1, data=azimuthal_data)
        group.create_dataset(dataset_name_var2, data=stage_positions)
        
        # Optionally, add attributes to each dataset
        group[dataset_name_var1].attrs['description'] = f'Azimuthal averaged data from run {run_number}'
        group[dataset_name_var2].attrs['description'] = f'Stage positions from run {run_number}'

    print(f"Run {run_number} data saved successfully.")
    return


def read_individual_run(file_name, group_name, run_number):
    """
    Allows you to read a specific run from an h5 file. 
    
    ARGUMENTS:
    
    file_name (str):
        file name or path that holds the data of interest
    group_name (str):
        group subset within the file
    run_number (int):
        run of interest
        
    RETURNS:
    
    I_data (2d array):
        azimuthally averaged scattering data for the specified run number
    stage_data (1d array):
        stage positions for the specified run number
    """

    with h5py.File(file_name, 'r') as f:
        group = f[group_name]
        
        # Create unique dataset names for each run
        dataset_name_var1 = f'I_run_{run_number}'
        dataset_name_var2 = f'stage_positions_run_{run_number}'
        
        if dataset_name_var1 not in group or dataset_name_var2 not in group:
            print(f"Error: Run {run_number} data not found in group '{group_name}'.")
            return None, None
        
        # Load data for var1 and var2
        I_data = group[dataset_name_var1][:]
        stage_data = group[dataset_name_var2][:]
        
        # Optionally, print attributes
        attr_var1 = group[dataset_name_var1].attrs.get('description', 'No description')
        attr_var2 = group[dataset_name_var2].attrs.get('description', 'No description')
        print(f"Attributes for var1_run_{run_number}: {attr_var1}")
        print(f"Attributes for var2_run_{run_number}: {attr_var2}")
        
    return I_data, stage_data


def read_combined_data(file_name, group_name):
    """Reads in and concatenates all the data within a group from an h5 file.
    
    ARGUMENTS:
    
    file_name (str):
        file name or path that holds the data of interest
    group_name (str):
        group subset within the file
        
    RETURNS:
    
    I_data (2d array):
        azimuthally averaged scattering data for the specified group (all runs combined)
    stage_data (1d array):
        stage positions for the specified group (all runs combined)

    """

    with h5py.File(file_name, 'r') as f:
        group = f[group_name]
        
        # Collect all datasets for var1 and var2
        I_data_list = []
        stage_data_list = []
        
        for dataset_name in group.keys():
            if 'I' in dataset_name:
                I_data_list.append(group[dataset_name][:])
            elif 'stage' in dataset_name:
                stage_data_list.append(group[dataset_name][:])
        
        # Combine data for var1 and var2
        I_data = np.concatenate(I_data_list)
        stage_data = np.concatenate(stage_data_list)
        
    return I_data, stage_data

